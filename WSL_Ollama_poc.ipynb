{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stuffs\n",
    " \n",
    "import os\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "#from langchain_community.llms import Ollama\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.schema import Document\n",
    "from chromadb.config import Settings\n",
    "import fitz  #PyMuPDF for extracting images\n",
    "from PIL import Image\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_images_from_pdf(pdf_path, output_folder):\n",
    "    \"\"\"\n",
    "    Extract images from a PDF file and save them to the specified folder.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "        output_folder (str): Folder to save extracted images.\n",
    "\n",
    "    Returns:\n",
    "        list: Paths to the extracted images.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    image_paths = []\n",
    "\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        images = page.get_images(full=True)\n",
    "\n",
    "        for img_index, img in enumerate(images):\n",
    "            xref = img[0]\n",
    "            base_image = doc.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "            image_ext = base_image[\"ext\"]\n",
    "            image = Image.open(io.BytesIO(image_bytes))\n",
    "\n",
    "            # Save image\n",
    "            image_name = f\"{os.path.splitext(os.path.basename(pdf_path))[0]}_page{page_num + 1}_img{img_index + 1}.{image_ext}\"\n",
    "            image_path = os.path.join(output_folder, image_name)\n",
    "            image.save(image_path)\n",
    "            image_paths.append(image_path)\n",
    "\n",
    "    return image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunks_from_pdf_with_images(docs_folder, image_output_folder):\n",
    "    \"\"\"\n",
    "    Process PDF files to extract text and images, then create chunks.\n",
    "\n",
    "    Args:\n",
    "        docs_folder (str): Folder containing PDF files.\n",
    "        image_output_folder (str): Folder to save extracted images.\n",
    "\n",
    "    Returns:\n",
    "        list: Combined text and image chunks.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(image_output_folder):\n",
    "        os.makedirs(image_output_folder)\n",
    "\n",
    "    Docs = []\n",
    "\n",
    "    for filename in os.listdir(docs_folder):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            file_path = os.path.join(docs_folder, filename)\n",
    "            print(f\"Processing file: {file_path}\")\n",
    "\n",
    "            # Load the document\n",
    "            loader = PyPDFLoader(file_path)\n",
    "            data = loader.load()\n",
    "\n",
    "            # Split the text into chunks\n",
    "            text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "            text_chunks = text_splitter.split_documents(data)\n",
    "            Docs.extend(text_chunks)\n",
    "\n",
    "            # Extract images and add their paths as chunks\n",
    "            image_paths = extract_images_from_pdf(file_path, image_output_folder)\n",
    "            for image_path in image_paths:\n",
    "                Docs.append({\"type\": \"image\", \"content\": image_path})\n",
    "\n",
    "    return Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunks_from_pdf(docs_folder):\n",
    "    Docs = []\n",
    "    # Iterate through all PDF files in the 'docs' folder\n",
    "    for filename in os.listdir(docs_folder):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            file_path = os.path.join(docs_folder, filename)\n",
    "            print(f\"Processing file: {file_path}\")\n",
    "\n",
    "            # Load the document\n",
    "            loader = PyPDFLoader(file_path)\n",
    "            data = loader.load()\n",
    "\n",
    "            # Split the document into chunks\n",
    "            text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "            splits = text_splitter.split_documents(data)\n",
    "\n",
    "            # Add the splits to the main list\n",
    "            Docs.extend(splits)\n",
    "    return Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_codedocs_from_folder(project_folder):\n",
    "    CodeDocs = []\n",
    "\n",
    "    # Traverse the folder structure\n",
    "    for root, dirs, files in os.walk(project_folder):\n",
    "        for file in files:\n",
    "            if file.endswith(\".h\") or file.endswith(\".c\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                print(f\"Processing file: {file_path}\")\n",
    "\n",
    "                # Read the file content\n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                        file_content = f.read()\n",
    "\n",
    "                    # Split the content into chunks\n",
    "                    text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=0)\n",
    "                    splits = text_splitter.split_text(file_content)\n",
    "\n",
    "                    # Add metadata and content to CodeDocs\n",
    "                    for i, chunk in enumerate(splits):\n",
    "                        CodeDocs.append(Document(\n",
    "                            page_content=chunk,\n",
    "                            metadata={\n",
    "                                \"file_name\": file,\n",
    "                                \"file_path\": file_path,\n",
    "                                \"chunk_index\": i\n",
    "                            }\n",
    "                        ))\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading file {file_path}: {e}\")\n",
    "\n",
    "    return CodeDocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ReqDocs/1340874- CCRack_mpci_Safety_NXP_SwRS-20241115_113100793.pdf\n",
      "Processing file: ReqDocs/461624- CCRack_mpci_EnvelopeB_SysRS-20241115_114428860.pdf\n",
      "Processing file: ReqDocs/Component_Specification_Modular_Power_Compute_Interlayer_MPCI_V1.1.pdf\n",
      "Processing file: ReferenceDocs/S32G_SAF_EMCEM_UM.pdf\n",
      "Processing file: ReferenceDocs/SAF.pdf\n",
      "Processing file: ReferenceDocs/S32G_SAF_IA_UM.pdf\n",
      "Processing file: ReferenceDocs/S32SAFPB.pdf\n",
      "Processing file: ReferenceDocs/RTD_THERMAL_UM.pdf\n",
      "Processing file: ReferenceDocs/TP-S32-SAFETY-SOFTWARE-TECHNICAL-OVERVIEW.pdf\n",
      "Processing file: ReferenceDocs/RTD_THERMAL_IM.pdf\n",
      "Processing file: ReferenceUMLDesignDocs/PlantUML_Language_Reference_Guide_en.pdf\n"
     ]
    }
   ],
   "source": [
    "# Define the folder containing the documents\n",
    "Requirement_docs_folder = \"ReqDocs/\"\n",
    " \n",
    "Reference_docs_folder = \"ReferenceDocs/\"\n",
    " \n",
    "#Code_folder = \"ReferenceCode/\"\n",
    " \n",
    "RefUMLDesignFolder = \"ReferenceUMLDesignDocs/\"\n",
    "ImageOutputFolder = \"ExtractedImages/\"\n",
    " \n",
    "reqDocs = create_chunks_from_pdf(Requirement_docs_folder)\n",
    "refDocs = create_chunks_from_pdf(Reference_docs_folder)\n",
    "#refCode = create_codedocs_from_folder(Code_folder)\n",
    "refDesgn = create_chunks_from_pdf(RefUMLDesignFolder)\n",
    "feature = \"thermal monitoring unit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Chroma Settings\n",
    "bge_client_settings_reqs = Settings(\n",
    "    chroma_api_impl=\"chromadb.api.segment.SegmentAPI\",  # Default API implementation\n",
    "    is_persistent=True,  # Enable persistence to save embeddings\n",
    "    persist_directory=\"./persistent_dir_embeddings_reqs\",  # Path for storage; ensure it exists\n",
    ")\n",
    "\n",
    "bge_client_settings_refs = Settings(\n",
    "    chroma_api_impl=\"chromadb.api.segment.SegmentAPI\",  # Default API implementation\n",
    "    is_persistent=True,  # Enable persistence to save embeddings\n",
    "    persist_directory=\"./persistent_dir_embeddings_refs\",  # Path for storage; ensure it exists\n",
    ")\n",
    "\n",
    "bge_client_settings_UML = Settings(\n",
    "    chroma_api_impl=\"chromadb.api.segment.SegmentAPI\",  # Default API implementation\n",
    "    is_persistent=True,  # Enable persistence to save embeddings\n",
    "    persist_directory=\"./persistent_dir_embeddings_UML\",  # Path for storage; ensure it exists\n",
    ")\n",
    "\n",
    "bge_model_local_embeddings = OllamaEmbeddings(model=\"bge-m3\")\n",
    "\n",
    "bge_model_vectorstore_reqs = Chroma.from_documents(documents=reqDocs, embedding=bge_model_local_embeddings, client_settings=bge_client_settings_reqs)\n",
    "bge_model_vectorstore_refs = Chroma.from_documents(documents=refDocs, embedding=bge_model_local_embeddings, client_settings=bge_client_settings_refs)\n",
    "bge_model_vectorstore_UML = Chroma.from_documents(documents=refDesgn, embedding=bge_model_local_embeddings, client_settings=bge_client_settings_UML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Chroma Settings\n",
    "snowflakee_client_settings_reqs = Settings(\n",
    "    chroma_api_impl=\"chromadb.api.segment.SegmentAPI\",  # Default API implementation\n",
    "    is_persistent=True,  # Enable persistence to save embeddings\n",
    "    persist_directory=\"./persistent_dir_embeddings_reqs\",  # Path for storage; ensure it exists\n",
    ")\n",
    "\n",
    "snowflakee_client_settings_refs = Settings(\n",
    "    chroma_api_impl=\"chromadb.api.segment.SegmentAPI\",  # Default API implementation\n",
    "    is_persistent=True,  # Enable persistence to save embeddings\n",
    "    persist_directory=\"./persistent_dir_embeddings_refs\",  # Path for storage; ensure it exists\n",
    ")\n",
    "\n",
    "snowflakee_client_settings_UML = Settings(\n",
    "    chroma_api_impl=\"chromadb.api.segment.SegmentAPI\",  # Default API implementation\n",
    "    is_persistent=True,  # Enable persistence to save embeddings\n",
    "    persist_directory=\"./persistent_dir_embeddings_UML\",  # Path for storage; ensure it exists\n",
    ")\n",
    "\n",
    "snowflakee_model_local_embeddings = OllamaEmbeddings(model=\"snowflake-arctic-embed2\")\n",
    "\n",
    "snowflakee_model_vectorstore_reqs = Chroma.from_documents(documents=reqDocs, embedding=snowflakee_model_local_embeddings, client_settings=snowflakee_client_settings_reqs)\n",
    "snowflakee_model_vectorstore_refs = Chroma.from_documents(documents=refDocs, embedding=snowflakee_model_local_embeddings, client_settings=snowflakee_client_settings_refs)\n",
    "snowflakee_model_vectorstore_UML = Chroma.from_documents(documents=refDesgn, embedding=snowflakee_model_local_embeddings, client_settings=snowflakee_client_settings_UML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistralmodel = ChatOllama(\n",
    "    model=\"mistral\",\n",
    "    temperature=0.2,\n",
    "    top_k=20,\n",
    "    top_p=0.7,\n",
    "    repeat_last_n=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama2model = ChatOllama(\n",
    "    model=\"llama3.2\",\n",
    "    temperature=0.2,\n",
    "    top_k=20,\n",
    "    top_p=0.7,\n",
    "    repeat_last_n=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DeepseekModel = ChatOllama(\n",
    "    model=\"deepseek-coder:6.7b\",\n",
    "    temperature=0.2,\n",
    "    top_k=20,\n",
    "    top_p=0.7,\n",
    "    repeat_last_n=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided **Requirement_Documents**, I have identified the following requirements:\n",
      "### **Functional Requirements**\n",
      "- FR1: The system shall monitor the SOC temperature to ensure it remains within a safe operating range.\n",
      "- FR2: The system shall trigger an alert when the SOC temperature exceeds the safe operating range.\n",
      "- FR3: The system shall monitor the SOC temperature continuously and update the temperature reading in real-time.\n",
      "- FR4: The system shall monitor the SOC temperature using a reliable and accurate sensor.\n",
      "### **Non-Functional Requirements**\n",
      "- NFR1: The system shall respond to user input within 2 seconds.\n",
      "- NFR2: The system shall maintain a minimum uptime of 99.9% without any downtime for maintenance.\n",
      "- NFR3: The system shall ensure data security and confidentiality by implementing encryption and access controls.\n",
      "### **Constraints & Dependencies**\n",
      "- C1: The system shall be designed to operate within a temperature range of -20°C to 80°C.\n",
      "- D1: The system shall be integrated with the existing monitoring system to ensure seamless data exchange.\n",
      "### **Additional Notes**\n",
      "- The system shall be designed to meet the requirements of the International Electrotechnical Commission (IEC) standards for temperature monitoring.\n",
      "- The system shall be tested and validated to ensure it meets the required performance and reliability standards.\n",
      "Please note that the above requirements are extracted from the provided **Requirement_Documents** and may not be a comprehensive list of all requirements for the system.\n"
     ]
    }
   ],
   "source": [
    "# Prompt Template\n",
    "RAG_TEMPLATE = \"\"\"\n",
    "You are an expert systems analyst specializing in requirements engineering. Your task is to summarize the requirements from the provided **Requirement_Documents**.\n",
    "\n",
    "### **Instructions:**\n",
    "- Identify and categorize all **Functional** and **Non-Functional** requirements.\n",
    "- Highlight any **constraints, dependencies, or assumptions** that may impact system design.\n",
    "- Ignore unrelated or ambiguous information.\n",
    "- Ensure consistency by following the below structured output format:\n",
    "\n",
    "### **Output Format:**\n",
    "1. **Functional Requirements**  \n",
    "   - FR1: [Requirement description]  \n",
    "   - FR2: [Requirement description]  \n",
    "\n",
    "2. **Non-Functional Requirements**  \n",
    "   - NFR1: [Requirement description]  \n",
    "   - NFR2: [Requirement description]  \n",
    "\n",
    "3. **Constraints & Dependencies**  \n",
    "   - C1: [Constraint description]  \n",
    "   - D1: [Dependency description] \n",
    "\n",
    "### **Requirement_Documents:**\n",
    "<Requirement_Documents>\n",
    "{Requirement_Documents}\n",
    "</Requirement_Documents>\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Define the ChatPromptTemplate\n",
    "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)\n",
    "\n",
    "# Define RAG pipeline function\n",
    "def extract_requirements(feature: str, model):\n",
    "    \"\"\"\n",
    "    Processes requirement documents and extracts relevant requirements for the specified feature.\n",
    "    \n",
    "    Parameters:\n",
    "        feature (str): The feature for which requirements need to be extracted.\n",
    "        model: The LLM model used for processing.\n",
    "\n",
    "    Returns:\n",
    "        str: Formatted response with extracted requirements.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Define chain for processing requirement documents\n",
    "        chain = (\n",
    "            RunnablePassthrough.assign(\n",
    "                Requirement_Documents=lambda input: format_docs(input[\"Requirement_Documents\"])\n",
    "            )\n",
    "            | rag_prompt\n",
    "            | model\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "        # Query for extracting requirements\n",
    "        question = (\n",
    "            f\"Extract and summarize all requirements related to the {feature}. \"\n",
    "            \"Ensure all critical details, dependencies, and constraints are captured.\"\n",
    "        )\n",
    "\n",
    "        # Perform similarity search on vector store\n",
    "        requirement_chunks = bge_model_vectorstore_reqs.similarity_search(question)\n",
    "\n",
    "        # Run the chain\n",
    "        response = chain.invoke({\n",
    "            \"Requirement_Documents\": requirement_chunks\n",
    "        })\n",
    "\n",
    "        return format_response(response)\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Function to clean up and format response\n",
    "def format_response(response: str) -> str:\n",
    "    \"\"\"\n",
    "    Formats the LLM-generated response by ensuring consistent paragraph spacing and structure.\n",
    "\n",
    "    Parameters:\n",
    "        response (str): Raw text response from the model.\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned and structured response.\n",
    "    \"\"\"\n",
    "    # Insert line breaks before code blocks and normalize spacing\n",
    "    formatted_response = re.sub(r\"(\\n`.*?`)\", r\"\\n\\n\\1\", response)  # Ensure spacing for code blocks\n",
    "    formatted_response = re.sub(r\"(\\n\\n)\", r\"\\n\", formatted_response)  # Normalize paragraph spacing\n",
    "\n",
    "    return formatted_response\n",
    "\n",
    "# Run the extraction process\n",
    "requirements_output = extract_requirements(feature=\"Thermal Monitoring Unit\", model=llama2model)\n",
    "\n",
    "# Print formatted output\n",
    "print(requirements_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided Reference Documents, I have identified the following API-specific details that match the Extracted Requirements:\n",
      "### **Extracted Requirement:** The system shall monitor the SOC temperature to ensure it remains within a safe operating range.\n",
      "   - **Matching API Function(s):** `get_temperature` and `set_threshold`\n",
      "   - **Parameters:** `site_id` (temperature sensor site), `threshold_value` (temperature threshold)\n",
      "   - **Protocol Used:** None\n",
      "   - **Constraints/Performance Notes:** The system shall ensure that the temperature reading is accurate and reliable.\n",
      "   - **Confidence Level:** High\n",
      "   - **Missing Details:** None\n",
      "### **Extracted Requirement:** The system shall monitor the SOC temperature to detect any anomalies or changes.\n",
      "   - **Matching API Function(s):** `get_temperature` and `set_threshold`\n",
      "   - **Parameters:** `site_id` (temperature sensor site), `threshold_value` (temperature threshold)\n",
      "   - **Protocol Used:** None\n",
      "   - **Constraints/Performance Notes:** The system shall send notifications or alerts when the temperature exceeds the set threshold.\n",
      "   - **Confidence Level:** High\n",
      "   - **Missing Details:** None\n",
      "### **Extracted Requirement:** The system shall enable and disable monitoring sites (temperature sensors) as needed.\n",
      "   - **Matching API Function(s):** `enable_monitoring_site` and `disable_monitoring_site`\n",
      "   - **Parameters:** `site_id` (temperature sensor site)\n",
      "   - **Protocol Used:** None\n",
      "   - **Constraints/Performance Notes:** The system shall ensure that the monitoring site is enabled or disabled correctly.\n",
      "   - **Confidence Level:** High\n",
      "   - **Missing Details:** None\n",
      "### **Extracted Requirement:** The system shall enable and disable thresholds that generate interrupts.\n",
      "   - **Matching API Function(s):** `set_threshold`\n",
      "   - **Parameters:** `threshold_value` (temperature threshold)\n",
      "   - **Protocol Used:** None\n",
      "   - **Constraints/Performance Notes:** The system shall ensure that the threshold is set correctly.\n",
      "   - **Confidence Level:** High\n",
      "   - **Missing Details:** None\n",
      "### **Extracted Requirement:** The system shall retrieve the current temperature at each site, or historical minimums or maximums.\n",
      "   - **Matching API Function(s):** `get_temperature`\n",
      "   - **Parameters:** `site_id` (temperature sensor site)\n",
      "   - **Protocol Used:** None\n",
      "   - **Constraints/Performance Notes:** The system shall ensure that the temperature reading is accurate and reliable.\n",
      "   - **Confidence Level:** High\n",
      "   - **Missing Details:** None\n",
      "### **Extracted Requirement:** The system shall provide historical minimums or maximums for each temperature sensor site.\n",
      "   - **Matching API Function(s):** `get_temperature_history`\n",
      "   - **Parameters:** `site_id` (temperature sensor site)\n",
      "   - **Protocol Used:** None\n",
      "   - **Constraints/Performance Notes:** The system shall ensure that the historical data is accurate and reliable.\n",
      "   - **Confidence Level:** High\n",
      "   - **Missing Details:** None\n",
      "Note that the API functions and parameters listed above are based on the provided Reference Documents and may not be an exhaustive list of all possible API functions and parameters. Additionally, the confidence level and missing details sections are subjective and based on my analysis of the provided information.\n"
     ]
    }
   ],
   "source": [
    "# Updated Prompt Template using Extracted Requirements\n",
    "RAG_TEMPLATE = \"\"\"\n",
    "You are an expert in Software / System Design. You excel in understanding the Reference Documents and the provided Requirements.\n",
    "Your task is to extract the **API-specific details** from the provided **Reference_Documents**, understand them and map them to the given **Extracted_Requirements**.\n",
    "\n",
    "### **Instructions:**\n",
    "- Identify relevant **API functions**, parameters, protocols, and constraints that match the **Extracted_Requirements**.\n",
    "- Clearly link each API function to the corresponding requirements.\n",
    "- If no matching API is found for a requirement, state \"No direct match found\" and suggest alternatives if applicable.\n",
    "- Use the following structured output format:\n",
    "\n",
    "### **Output Format:**\n",
    "1. **Extracted Requirement:** [Requirement Detail]\n",
    "   - **Matching API Function(s):** [Function Name]\n",
    "   - **Parameters:** [Parameter List]\n",
    "   - **Protocol Used:** [Protocol Name (if applicable)]\n",
    "   - **Constraints/Performance Notes:** [Relevant Details]\n",
    "   - **Confidence Level:** [High / Medium / Low]\n",
    "   - **Missing Details:** [Any missing or unclear information]\n",
    "\n",
    "### **Input Documents:**\n",
    "- **Extracted Requirements:**  \n",
    "  <Extracted_Requirements>\n",
    "  {Extracted_Requirements}\n",
    "  </Extracted_Requirements>\n",
    "\n",
    "- **Reference Documents:**  \n",
    "  <Reference_Documents>\n",
    "  {Reference_Documents}\n",
    "  </Reference_Documents>\n",
    "\"\"\"\n",
    "\n",
    "# Define the ChatPromptTemplate\n",
    "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)\n",
    "\n",
    "# Define function to extract API details using extracted requirements\n",
    "def extract_api_details(extracted_requirements: str, reference_docs_vectorstore, model):\n",
    "    \"\"\"\n",
    "    Extracts API-specific details relevant to given extracted requirements from vendor reference documents.\n",
    "    \n",
    "    Parameters:\n",
    "        extracted_requirements (str): The requirements extracted from the previous step.\n",
    "        reference_docs_vectorstore: Vector store used for document retrieval.\n",
    "        model: The LLM model used for processing.\n",
    "\n",
    "    Returns:\n",
    "        str: Formatted response containing extracted API details.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Define a dynamic query based on the extracted requirements\n",
    "        question = (\n",
    "            f\"Extract and summarize all API-related details that map to the following requirements: {extracted_requirements}. \"\n",
    "            \"Ensure all relevant API functions, parameters, protocols, and constraints are captured.\"\n",
    "        )\n",
    "\n",
    "        # Retrieve reference document chunks from the vector store\n",
    "        reference_chunks = reference_docs_vectorstore.similarity_search(question,10)\n",
    "        if not reference_chunks:\n",
    "            return f\"No relevant reference documents found for the provided requirements.\"\n",
    "\n",
    "        # Build the processing chain with dynamic assignment\n",
    "        chain = (\n",
    "            RunnablePassthrough.assign(\n",
    "                Reference_Documents=lambda input: format_docs(input[\"Reference_Documents\"]),\n",
    "                Extracted_Requirements=lambda input: input[\"Extracted_Requirements\"]\n",
    "            )\n",
    "            | rag_prompt\n",
    "            | model\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "        # Invoke the chain with both contexts\n",
    "        response = chain.invoke({\n",
    "            \"Reference_Documents\": reference_chunks,\n",
    "            \"Extracted_Requirements\": extracted_requirements\n",
    "        })\n",
    "\n",
    "        return format_response(response)\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Function to clean up and format the response\n",
    "def format_response(response: str) -> str:\n",
    "    \"\"\"\n",
    "    Formats the LLM-generated response by ensuring consistent paragraph spacing and structure.\n",
    "    \"\"\"\n",
    "    formatted_response = re.sub(r\"(\\n`.*?`)\", r\"\\n\\n\\1\", response)\n",
    "    formatted_response = re.sub(r\"(\\n\\n)\", r\"\\n\", formatted_response)\n",
    "    return formatted_response\n",
    "\n",
    "# Example usage\n",
    "extracted_requirements = requirements_output\n",
    "reference_output = extract_api_details(\n",
    "    extracted_requirements=extracted_requirements,\n",
    "    reference_docs_vectorstore=bge_model_vectorstore_refs,\n",
    "    model=llama2model\n",
    ")\n",
    "\n",
    "# Print formatted output\n",
    "print(reference_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ```plantuml\n",
      "@startuml\n",
      "skinparam lifelineStrategy solid\n",
      "skinparam style strictuml\n",
      "actor User\n",
      "actor Chip\n",
      "Chip --|> User: Interface\n",
      "Chip --|> API_Server: Communicate API Requests\n",
      "API_Server --|> Chip: API Response\n",
      "API_Server --|> Database: Store/Retrieve Data\n",
      "Database --|> API_Server: Data\n",
      "User ->> Chip: Enable Features\n",
      "User ->> Chip: Disable Features\n",
      "User ->> Chip: Get Status\n",
      "@enduml\n",
      "\n",
      "```\n",
      "In this diagram:\n",
      "- `User` represents the end-user interacting with the chip.\n",
      "- `Chip` represents the hardware device that communicates with the API server.\n",
      "- `API_Server` represents the server that handles API requests and responses.\n",
      "- `Database` represents the database where data is stored and retrieved.\n",
      "The arrows represent the interactions between these components. The `--|>` notation indicates that the component on the left sends a message to the component on the right, while the component on the right does not respond. The `->>` notation indicates that the component on the left sends a message to the component on the right, and the component on the right may respond.\n",
      "The `User` interacts with the `Chip` to enable/disable features, get the status, and the `Chip` communicates with the `API_Server` to send and receive data. The `Chip` also communicates with the `Database` to store and retrieve data.\n",
      "This diagram is in PlantUML format and follows the design guidelines provided, using the `nosolid` lifeline strategy for the lifelines and the `strictuml` style for the arrowheads.\n"
     ]
    }
   ],
   "source": [
    "# Updated Prompt Template using PlantUML and dynamic feature injection\n",
    "RAG_TEMPLATE = \"\"\"\n",
    "You are a seasoned software architect specializing in UML-based software design. You have been provided with:\n",
    "1. Chip vendor API details that align with the following **Reference_docs**.\n",
    "<Reference_docs>\n",
    "{Reference_docs}\n",
    "</Reference_docs>\n",
    "2. Guidelines for generating UML Designs that allign with **Design Guidelines:**  \n",
    "<Design_Guidelines>\n",
    "{Design_Guidelines}\n",
    "</Design_Guidelines>\n",
    "\n",
    "Bring out your understanding of **Reference_docs** and use the UML Design Guidelines from **Design_Guidelines**, generate a comprehensive UML Diagram that meets all technical constraints.\n",
    "\n",
    "**Instructions:**\n",
    "- Identify the main components and interactions based on the provided requirements and design guidelines.\n",
    "- Output the diagram in PlantUML format.\n",
    "- Clearly annotate the diagram with inline comments explaining key design decisions.\n",
    "- Output only the PlantUML code without any extra text.\n",
    "\"\"\"\n",
    "\n",
    "# Create the ChatPromptTemplate from the above template\n",
    "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)\n",
    "\n",
    "def generate_plantuml_diagram(\n",
    "    extracted_api_details: str,\n",
    "    design_guidelines_vectorstore,\n",
    "    model, \n",
    "    UML_Diagram: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate a UML sequence diagram in PlantUML format based on extracted API details and design guidelines.\n",
    "    \n",
    "    Args:\n",
    "        extracted_api_details (str): Extracted API details (as text) from previous steps.\n",
    "        design_guidelines_vectorstore: Vector store instance to retrieve design guidelines.\n",
    "        feature (str): The feature for which the UML diagram is to be generated.\n",
    "        model: The LLM model used in the pipeline.\n",
    "    \n",
    "    Returns:\n",
    "        str: The formatted PlantUML code for the UML sequence diagram.\n",
    "    \"\"\"\n",
    "    # Query to retrieve design guidelines from the vector store.\n",
    "    query = f\"Extract the guidelines for a {UML_Diagram} that meet technical constraints.\"\n",
    "    design_guidelines_docs = design_guidelines_vectorstore.similarity_search(query)\n",
    "    \n",
    "    if not design_guidelines_docs:\n",
    "        return f\"No relevant design guidelines found for: {UML_Diagram}\"\n",
    "    \n",
    "    # Build the processing chain with dynamic assignment.\n",
    "    chain = (\n",
    "        RunnablePassthrough.assign(\n",
    "            Design_Guidelines=lambda input: format_docs(input[\"Design_Guidelines\"]),\n",
    "            Reference_docs=lambda input: extracted_api_details\n",
    "        )\n",
    "        | rag_prompt\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    # Invoke the chain with the required contexts.\n",
    "    response = chain.invoke({\n",
    "        \"Design_Guidelines\": design_guidelines_docs,\n",
    "        \"Reference_docs\": extracted_api_details\n",
    "    })\n",
    "    \n",
    "    # Clean up and format the PlantUML output.\n",
    "    formatted_response = re.sub(r\"(\\n`.*?`)\", r\"\\n\\n\\1\", response)\n",
    "    formatted_response = re.sub(r\"(\\n\\n)\", r\"\\n\", formatted_response)\n",
    "    return formatted_response\n",
    "\n",
    "# Example usage in a Jupyter Notebook:\n",
    "# (Ensure that 'extracted_api_details', 'bge_model_vectorstore_UML', and 'mistralmodel' are defined in your notebook.)\n",
    "#\n",
    "extracted_api_details = reference_output\n",
    "diagram_of_interest = \"Sequence Diagram\"\n",
    "vector_store = bge_model_vectorstore_UML\n",
    "#\n",
    "plantuml_output = generate_plantuml_diagram(\n",
    "    extracted_api_details,\n",
    "    vector_store,\n",
    "    mistralmodel,\n",
    "    diagram_of_interest\n",
    ")\n",
    "print(plantuml_output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myJupyterKernel",
   "language": "python",
   "name": "myjupyterkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
